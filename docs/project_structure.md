# ğŸ§± MVP Tech Stack (Example)
* Backend: FastAPI
* NLP: Clinical NER + LLM classification
* Storage: Postgres
* Security: basic HIPAA hygiene
* UI: simple form + checklist view


# 1ï¸âƒ£ High-Level Architecture (MVP) (View in edit mode)
[React Web App]
    |
    |  upload chart + select CPT + payer (optional)
    v
[FastAPI Backend]
    |
    |-- Document ingestion (PDF / text)
    |-- Evidence detection (LLM + rules)
    |-- Missing-info checklist
    |-- Justification assembly
    |
    v
[JSON PA Readiness Report]

TODO, need and upload and results page.

# 2ï¸âƒ£ Backend Skeleton (FastAPI)
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ pa.py
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ security.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ pa_models.py
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ ingestion.py
â”‚   â”‚   â”œâ”€â”€ evidence.py
â”‚   â”‚   â”œâ”€â”€ readiness.py
â”‚   â”‚   â”œâ”€â”€ justification.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ text.py
â”‚   â””â”€â”€ requirements.py

## Next Steps
### Step 1: Start with .txt uploads

Keep ingestion simple.

### Step 2: Normalize text

Run everything through normalize_text().

### Step 3: Run evidence detectors

Check:
* conservative therapy
* duration
* failed treatments
* severity
* imaging

### Step 4: Validate outputs manually

Ask:
â€œDoes this checklist match what a clinic would expect?â€

If yes â€” youâ€™re on the right track.


### High-level Clarification
Chart Note (PDF / Text)
        |
        v
[1] Clinical Fact Extraction (NO RAG)
        |
        v
Structured Evidence JSON
(symptoms, duration, imaging, PT, etc.)
        |
        v
[2] Policy Criteria Retrieval (RAG)
(payer + CPT â†’ criteria snippets)
        |
        v
[3] Criteria Matching
(evidence vs criteria checklist)
        |
        v
[4] Output Generation
- readiness
- missing items
- justification (uses RAG text)

### Clarifying how AI is used here.
| Layer                | What it Understands          | AI Model Role              |
| -------------------- | ---------------------------- | -------------------------- |
| **Clinical Reality** | What happened to the patient | **Qwen extracts facts**    |
| **Insurance Rules**  | What the payer requires      | **RAG retrieves criteria** |

### Iteration upon iteration.
Chart Note
   |
   v
[1] Evidence Extraction (Qwen, NO RAG)
   |   â†’ Turns messy note into structured medical facts
   v
Structured Evidence JSON
   |
   v
[2] Policy Retrieval (RAG)
   |   â†’ Fetches payer rules from your policy vector DB
   v
Policy Criteria Snippets
   |
   v
[3] Criteria Matching (Pure Logic)
   |   â†’ Compares evidence JSON vs policy checklist
   v
Readiness Score + Missing Items
   |
   v
[4] Justification Generation (LLM + RAG text)
       â†’ Uses evidence + retrieved policy language

### âš™ï¸ Important: RAG Is Only for Policies
### Do NOT put chart notes into this vector DB.

Your vector DB should ONLY contain:
* Payer medical policies
* Coverage determination documents
* Clinical criteria bullet lists
* If you mix charts in, youâ€™ll get garbage retrieval.

### User's Perspective
1. Wrangle all the docs/pdfs/text files they can find and dump them into the evidence extraction code and create structured json object.
2. Once they have all that stuff they need to figure out which payer policy/criteria they're filling out for their patient.
3. Select the correct codes/policy/insurance company. (The indecies should already be built for them to use...)
4. They should receive a nice document with PA readiness check out of 100 with certain things highlighted and is usable.


#### Behind the scenes

* One-time setup (offline)
* Load policy PDFs
* Split into chunks (300â€“800 tokens)
* Embed chunks

Store in vector DB with metadata:
{
  "payer": "BCBS",
  "cpt_codes": ["62323", "64483"],
  "body_part": "lumbar_spine",
  "source": "BCBS_Lumbar_ESI_2025.pdf"
}

### FastAPI call
User uploads chart
        â†“
Qwen extracts facts
        â†“
You query RAG with payer + CPT
        â†“
Vector DB returns matching policy rules
        â†“
Logic compares facts vs rules
        â†“
LLM writes justification using those rules

Component	                Changes When?	        Needs Retraining?
Clinical extractor (Qwen)	Rarely	                âŒ
Policy RAG DB	                Every policy update	âœ… Just re-embed
Matching logic	                When rules change	âŒ
Justification writer	        Never	                âŒ

#### Another Mental Model
Step	                Brain Used	        Data Source
Evidence extraction	Qwen	                Chart
Policy retrieval	Embeddings + Vector DB	Policy PDFs
Readiness scoring	Deterministic logic	Evidence + Policy
Justification writing	LLM	                Evidence + Policy text

### Api endpoints
/api/ask_question          [POST]   - Query RAG system
/api/build_index           [POST]   - Build new index
/api/delete_index/{name}   [DELETE] - Delete index
/api/list_indexes          [GET]    - List all indexes

/api/upload_document       [POST]   - Upload document
/api/list_uploaded_docs    [GET]    - List documents
/api/delete_uploaded_doc/  [DELETE] - Delete document

/api/analyze               [POST]   - Analyze PA document (NEW)

### Backend App Structure
backend/app/
â”œâ”€â”€ main.py                 # Main app with all routers
â”œâ”€â”€ routers/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ rag.py             # RAG operations
â”‚   â”œâ”€â”€ documents.py       # Document management
â”‚   â””â”€â”€ prior_auth.py      # PA analysis (NEW)
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ ingestion.py
â”‚   â”œâ”€â”€ evidence.py
â”‚   â”œâ”€â”€ readiness.py
â”‚   â””â”€â”€ justification.py
â””â”€â”€ rag_pipeline/
    â””â”€â”€ scripts/
        â”œâ”€â”€ ask_question.py
        â””â”€â”€ build_index.py