# Orchestration Diagnostics

This directory contains diagnostic JSON files generated by the `/api/check_prior_auth` endpoint to help troubleshoot the PA readiness pipeline.

## File Naming Convention

Each diagnostic file is named: `{step}_{patient_chart_filename}.json`

Where `{step}` is one of:
- `01_raw_patient` - Raw patient chart extraction from LLM
- `02_raw_policy` - Raw policy rules extraction from RAG pipeline
- `03_normalized_patient` - Normalized patient evidence data
- `04_normalized_policy` - Normalized policy rules
- `05_evaluation` - Rule engine evaluation results
- `06_final_response` - Final API response

## How to Use

### Running the Endpoint

When you call the `/api/check_prior_auth` endpoint with a patient chart, it automatically saves diagnostic files for that request.

Example:
```bash
curl -X POST "http://localhost:8000/api/check_prior_auth" \
  -F "file=@patient_chart.pdf" \
  -F "payer=utah_medicaid" \
  -F "cpt=73721"
```

### Interpreting the Diagnostics

#### Step 1: Raw Patient Extraction (`01_raw_patient_*.json`)

This shows what the LLM extracted from the patient chart. Check:
- Is the extraction complete?
- Are all clinical facts captured?
- Check `_metadata.validation_passed` and `_metadata.hallucinations_detected`

#### Step 2: Raw Policy Extraction (`02_raw_policy_*.json`)

This shows what the RAG pipeline retrieved from the policy documents. Check:
- Are the coverage criteria relevant to the CPT code?
- Do the prerequisites match the payer's actual requirements?
- Review the `context` field to see which policy sections were retrieved

#### Step 3: Normalized Patient (`03_normalized_patient_*.json`)

This is the flattened patient data structure used by the rule engine. Check:
- Are all expected fields present?
- Are boolean values correctly interpreted (true/false)?
- Are numeric values properly converted (weeks, months, etc.)?

Key fields to verify:
```json
{
  "symptom_duration_months": 4,
  "pt_attempted": true,
  "pt_duration_weeks": 8,
  "nsaid_documented": true,
  "nsaid_failed": false,
  "imaging_documented": true,
  "imaging_months_ago": 0,
  "validation_passed": true,
  "hallucinations_detected": 0
}
```

#### Step 4: Normalized Policy (`04_normalized_policy_*.json`)

This is the rule structure that the rule engine will evaluate. Each rule has:
```json
{
  "id": "rule_identifier",
  "description": "Human-readable description",
  "logic": "all",  // or "any"
  "conditions": [
    {
      "field": "pt_attempted",
      "operator": "eq",
      "value": true
    }
  ]
}
```

Check:
- Are the field names in `conditions` matching the normalized patient fields?
- Are the operators correct? (eq, gte, lte, etc.)
- Are the threshold values reasonable?

#### Step 5: Rule Evaluation (`05_evaluation_*.json`)

This shows the detailed evaluation results for each rule. This is the MOST IMPORTANT file for debugging.

Structure:
```json
{
  "results": [
    {
      "rule_id": "physical_therapy_requirement",
      "description": "Physical therapy must be attempted...",
      "met": true,
      "logic": "all",
      "condition_details": [
        {
          "condition": "pt_attempted eq True",
          "patient_value": true,
          "met": true
        },
        {
          "condition": "pt_duration_weeks gte 6",
          "patient_value": 8,
          "met": true
        }
      ]
    }
  ],
  "all_criteria_met": false,
  "total_rules": 5,
  "rules_met": 4,
  "rules_failed": 1
}
```

**What to look for:**
- Which rules failed? (`"met": false`)
- For failed rules, check `condition_details`:
  - What was the `patient_value`? (Is it null/missing?)
  - What was the required `condition`?
  - Did the comparison logic work correctly?

#### Step 6: Final Response (`06_final_response_*.json`)

This is the final API response sent to the client. Check:
- Does the `readiness_score` match the evaluation results?
- Are the `criteria` descriptions helpful?
- Do the `gaps` list make sense?
- Is the `verdict` appropriate for the score?

## Common Issues and Solutions

### Issue: All rules fail except one

**Symptom:** `05_evaluation` shows most rules failing unexpectedly.

**Check:**
1. Compare field names in `03_normalized_patient` vs `04_normalized_policy`
2. Look for typos or case mismatches
3. Check if patient values are null when they shouldn't be

### Issue: Rule engine can't find patient fields

**Symptom:** `condition_details` shows `"patient_value": null` for fields that exist.

**Cause:** Field name mismatch or nested structure issue.

**Solution:**
1. Check `03_normalized_patient` - is the field at the top level?
2. If the field is nested, the rule engine needs to use dot notation (e.g., `"field": "therapy.physical.weeks"`)

### Issue: Boolean comparisons failing

**Symptom:** Rules with `"operator": "eq"` and `"value": true` are failing even when patient data shows `true`.

**Check:**
1. Are the values actually booleans or strings? (true vs "true")
2. Is the comparison case-sensitive?

### Issue: Only "evidence quality" rule is evaluated

**Symptom:** `05_evaluation` shows only 1 rule instead of 5+.

**Cause:** Normalization may have failed, or policy extraction returned wrong structure.

**Solution:**
1. Check `02_raw_policy` - is `rules.coverage_criteria` populated?
2. Check `04_normalized_policy` - is the `rules` array populated?
3. Check server logs for normalization errors

## Next Steps

After reviewing the diagnostics:

1. Identify which step has incorrect data
2. If extraction is wrong (steps 1-2): Adjust LLM prompts or RAG configuration
3. If normalization is wrong (steps 3-4): Fix normalization functions in `app/normalization/`
4. If evaluation is wrong (step 5): Fix rule engine logic in `app/rules/rule_engine.py`
5. If response building is wrong (step 6): Fix response construction in `orchestration.py`

## Cleaning Up

Diagnostic files persist across runs. To clean up old diagnostics:

```bash
# From repository root
rm orchestration_diagnostics/*.json
```

Or use the API (if implemented):
```bash
curl -X DELETE "http://localhost:8000/api/diagnostics/clear"
```
